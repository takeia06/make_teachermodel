name: CI / CD (AnomalyGPT)

on:
  push:
    branches: [ main, develop ]
  pull_request:
  workflow_dispatch:

env:
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  PYTHONUNBUFFERED: "1"

jobs:
  lint_and_unit_cpu:
    name: Lint & Unit (CPU)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install minimal deps
        run: |
          python -m pip install -U pip
          pip install ruff black pytest

      - name: Lint (ruff + black --check)
        run: |
          ruff check .
          black --check .

      - name: Unit tests (CPU only; fast)
        run: |
          pytest -q -k "not gpu" || true

  gpu_smoke:
    name: Smoke (GPU, self-hosted)
    runs-on: [ self-hosted, gpu ]
    needs: lint_and_unit_cpu
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show GPU info
        run: nvidia-smi || true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install CUDA deps
        run: |
          python -m pip install -U pip wheel
          # Install a CUDA-enabled PyTorch (adjust CUDA version as needed)
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
          # Try your project deps if you have them (non-fatal here)
          if [ -f requirements.txt ]; then pip install -r requirements.txt || true; fi
          if [ -f requirements.gpu.txt ]; then pip install -r requirements.gpu.txt || true; fi

      - name: Optional: install MMDetection repo in editable mode
        run: |
          if [ -d "mmdetection" ]; then pip install -e mmdetection || true; fi

      - name: Smoke test (GPU path)
        env:
          SMOKE_CKPT: ${{ secrets.SMOKE_CKPT }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "import torch; assert torch.cuda.is_available(); print('CUDA available:', torch.cuda.get_device_name(0))"
          pytest -q -m gpu || true

  docker_build_and_publish:
    name: Build & Publish Docker (on tag)
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: .
          push: true
          tags: ghcr.io/${{ github.repository }}:latest,ghcr.io/${{ github.repository }}:${{ github.ref_name }}
          # If you have a CUDA base image, specify it within your Dockerfile.
