# === Phase1' POLISH（最終磨き） ===
out_dir: /home/takei/AnomalyGPT/outputs/refcoco_phase1p_v2
resume: ""
resume_full: full
save_every_steps: 0

num_workers: 0
pin_memory: true
ema_decay: 0.999   # ← 仕上げの安定寄り（trainerはCPU EMA）

train:
  batch_size: 4
  epochs: 30
  amp: true
  save_attn_vis: true
  save_every: 800
  log_interval: 100
  optimizer: "adamw"
  scheduler: "cosine"    # ← trainer側は lr_warmup_steps を別管理
  warmup_steps: 0    # ← メモ用（lr_warmup_steps が実際に参照される）

# SGDRを選ぶ場合に使用（scheduler: "sgdr" のときだけ）
sgdr:
  t0: 2
  t_mult: 2
  eta_min: 1.0e-6

use_amp: true
use_fp16: false          # bf16/AMP前提。必要ならtrueに

# 層別LR（安定寄り）
lr_head: 1.0e-3
lr_q_kv: 2.0e-4
lr_q_last: 5.0e-4
wd_head: 0.0
wd_body: 0.01
lr_warmup_steps: 1000    # ← 実際に使われるwarmupステップ

# 追加（任意）：focal係数を明示（trainerは無指定時に内部既定あり）
focal:
  w0: 0.30
  w1: 0.10
  decay_steps: 150

# 追加（任意）：MixUpを明示（Phase1では通常0でOK）
mixup_prob: 0.0
mixup_alpha: 0.2

model:
  pretrained_qformer: "/home/takei/AnomalyGPT/pretrained_qformer/instruct_qformer_only.pth"
  openclip_name: ViT-g-14
  openclip_pretrained: laion2b_s34b_b88k
  llm_name: "/home/takei/AnomalyGPT/pretrained_ckpt/vicuna_ckpt/7b_v1.5"
  load_vicuna: true
  max_txt_len: 128
  num_queries: 16
  proj_dim_in: 768
  proj_dim_out: 4096
  encoder_hidden_size: 1408     # ← Q-Former cross-attn K/V想定次元（instruct系に合わせる）
  text_cond_mode: "bias"        # "bias" or "bias+kv"
  num_text_kv: 4                # bias+kv時のみ有効（保守のため残しておく）

  patch: 14
  max_alpha: 0.85

loss_weights:
  attn: 0.55                    # ← わずかに下げてbox寄り
  box: 1.5
  contrast: 0.05
  lm: 0.0

aux_loss:
  soft_iou_w: 0.05
  lovasz_w: 0.05
  edge_bce_w: 0.0
  tv_w: 0.0

# Box重みのウォームアップ（trainer内でw_boxに反映）
box_warmup_steps: 500
box_target_weight: 0.6          # ← 仕上げでは0.5〜0.8の間で安定を見つつ

contrast:
  temperature: 0.07
  w0: 0.02
  w1: 0.10
  warmup_steps: 1500
  ramp_steps: 2000

dataset:
  train_json:
    - /home/takei/data/refcoco_data/refcoco_json/refcoco/RefCOCO_train_aug.json
    - /home/takei/data/refcoco_data/refcoco_json/refcocopp/RefCOCOplus_train_aug.json
    - /home/takei/data/refcoco_data/refcoco_json/refcocog/RefCOCOg_train_aug.json
  val_json:
    - /home/takei/data/refcoco_data/refcoco_json/refcoco/RefCOCO_val_aug.json
    - /home/takei/data/refcoco_data/refcoco_json/refcocopp/RefCOCOplus_val_aug.json
    - /home/takei/data/refcoco_data/refcoco_json/refcocog/RefCOCOg_val_aug.json
  img_root:
    train2014: /home/takei/data/refcoco_data/train2014
    val2014:   /home/takei/data/refcoco_data/val2014
    default:   /home/takei/data/refcoco_data
  supervision: mask
  long_side: 1008
  split: train
  use_answers: expand
  answer_key: answer
  text_key: text
  val_answer_mode: first
  augment:
    flip: true
    color_jitter: false
    color_jitter_params:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.05
    random_resized_crop:
      enabled: true
      scale: [0.8, 1.0]
      ratio: [0.9, 1.1]

eval:
  quantile_p: 0.80
  quantile_p0: 0.90

debug:
  sanity_steps: 300
  sanity_every: 0
  grad_clip: 1.0
  overfit_one_batch: false

logging:
  tensorboard: true

early_stop:
  metric: "val_iou_05"
  patience: 5
