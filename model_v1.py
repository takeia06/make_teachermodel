import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- GroundingDINO & ImageBind ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# ã“ã‚Œã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒæˆåŠŸã™ã‚‹ãŸã‚ã«ã¯ã€GroundingDINOã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
try:
    from GroundingDINO.groundingdino.util.inference import load_model
    from GroundingDINO.groundingdino.util.misc import NestedTensor
    from GroundingDINO.groundingdino.models.GroundingDINO.backbone.position_encoding import PositionEmbeddingSine
    from model.ImageBind.models.imagebind_model import imagebind_huge, ModalityType
    from model.ImageBind.data import load_and_transform_vision_data
except ImportError as e:
    print(f"Error importing modules: {e}")
    print("Please ensure GroundingDINO and its dependencies are correctly installed and accessible in your PYTHONPATH.")
    raise

# --- ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼å±¤ã®å®šç¾© ---
class AdapterLayer(nn.Module):
    def __init__(self, input_dim=1024, output_dim=256, sequence_length=256):
        super().__init__()
        self.projection = nn.Linear(input_dim, output_dim * sequence_length)
        self.output_dim = output_dim
        self.sequence_length = sequence_length
        self.layer_norm = nn.LayerNorm(output_dim)
        print(f"âœ… Adapter Layer initialized: maps ({input_dim}) -> ({sequence_length}, {output_dim})")

    def forward(self, features: torch.Tensor) -> torch.Tensor:
        projected = self.projection(features)
        reshaped = projected.view(-1, self.sequence_length, self.output_dim)
        return self.layer_norm(reshaped)

# --- æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®çµ„ã¿ç«‹ã¦ ---
class GroundingDINO_AnomalyGPT_v2(nn.Module):
    def __init__(self, grounding_dino_model, image_encoder, llm, llm_tokenizer, device='cuda'):
        super().__init__()
        self.device = device
        self.grounding_dino = grounding_dino_model
        self.image_encoder = image_encoder
        self.llm = llm
        self.llm_tokenizer = llm_tokenizer
        
        # --- å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«çµ±åˆ ---
        self.adapter = AdapterLayer().to(device)
        self.feature_projection = nn.Linear(256, self.llm.config.hidden_size).to(device)
        self.pos_embed_generator = PositionEmbeddingSine(num_pos_feats=128, temperature=20, normalize=True).to(device)
        
        print("âœ… GroundingDINO_AnomalyGPT_v2 architecture is ready.")

    def forward(self, image_path: str, text_prompt: str):
        """
        æ¨è«–ï¼ˆInferenceï¼‰ç”¨ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        """
        print("\n--- ğŸš€ Starting Inference Forward Pass ğŸš€ ---")
        
        with torch.no_grad():
            # 1. ImageBindã§ç”»åƒç‰¹å¾´ã‚’æŠ½å‡º
            vision_inputs = load_and_transform_vision_data([image_path], self.device)
            vision_outputs = self.image_encoder({ModalityType.VISION: vision_inputs})[ModalityType.VISION]
            image_features_1024d = vision_outputs[0]
            print(f"   1. ImageBind output shape: {image_features_1024d.shape}")

            # 2. Adapterã§ç‰¹å¾´é‡ã‚’GroundingDINOã®å½¢å¼ã«å¤‰æ›
            adapted_features_256d = self.adapter(image_features_1024d)
            print(f"   2. Adapter output shape: {adapted_features_256d.shape}")

            # 3. GroundingDINOã®Transformerã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’èåˆ
            # ... (å­¦ç¿’ãƒ­ã‚¸ãƒƒã‚¯ã¨ã¯åˆ¥ã®ã€æ¨è«–ç”¨ã®è©³ç´°ãªå‡¦ç†) ...
            # ã“ã®éƒ¨åˆ†ã¯æ¨è«–æ™‚ã«å®Ÿè£…ã—ã¾ã™ãŒã€ã¾ãšã¯å­¦ç¿’ã‚’å„ªå…ˆã—ã¾ã™
            
            # (ä»®ã®å‡ºåŠ›)
            fused_features = adapted_features_256d # æœ¬æ¥ã¯DINOã®Transformerã‹ã‚‰ã®å‡ºåŠ›
            print(f"   âœ… (Placeholder) Features fused. Shape: {fused_features.shape}")

            # 4. LLMã«å…¥åŠ›ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
            avg_features = fused_features.mean(dim=1)
            projected_for_llm = self.feature_projection(avg_features).unsqueeze(1)

            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦
            PROMPT_START = "###Human: <Img>"
            PROMPT_END   = "</Img> " + text_prompt + "###Assistant:"
            start_tokens = self.llm_tokenizer(PROMPT_START, return_tensors="pt")
            end_tokens = self.llm_tokenizer(PROMPT_END, return_tensors="pt")
            start_embeds = self.llm.get_input_embeddings()(start_tokens.input_ids.to(self.device))
            end_embeds = self.llm.get_input_embeddings()(end_tokens.input_ids.to(self.device))
            
            inputs_embeds = torch.cat([start_embeds, projected_for_llm, end_embeds], dim=1)
            
            # LLMã§å¿œç­”ç”Ÿæˆ
            outputs = self.llm.generate(
                inputs_embeds=inputs_embeds, max_new_tokens=128, do_sample=False,
                pad_token_id=self.llm_tokenizer.pad_token_id
            )
            response_tokens = outputs[0][inputs_embeds.shape[1]:]
            response = self.llm_tokenizer.decode(response_tokens, skip_special_tokens=True)

        print("--- âœ… Inference Forward Pass Complete ---")
        return response

        # model_v1.py ã® GroundingDINO_AnomalyGPT_v2 ã‚¯ãƒ©ã‚¹å†…ã«è¿½åŠ 

    def forward_train(self, images: NestedTensor, text_prompts: list[str]):
        """
        å­¦ç¿’ï¼ˆTrainingï¼‰ç”¨ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        """
        print("\n--- ğŸ’¡ Starting Training Forward Pass ğŸ’¡ ---")
        
        # 1. ImageBindã§ç”»åƒç‰¹å¾´ã‚’æŠ½å‡º
        #    (æ³¨: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å—ã‘å–ã‚‹'images'ã¯æ—¢ã«ãƒ†ãƒ³ã‚½ãƒ«ãªã®ã§ã€ãƒ‘ã‚¹ã§ã¯ãªããã®ã¾ã¾ä½¿ã„ã¾ã™)
        with torch.no_grad(): # ImageBindã®é‡ã¿ã¯å›ºå®š
            vision_outputs = self.image_encoder({ModalityType.VISION: images.tensors})[ModalityType.VISION]
            image_features_1024d = vision_outputs[0]

        # 2. Adapterã§ç‰¹å¾´é‡ã‚’GroundingDINOã®å½¢å¼ã«å¤‰æ›
        adapted_features_256d = self.adapter(image_features_1024d)

        # 3. GroundingDINOã®Transformerã«å…¥åŠ›ã™ã‚‹ãŸã‚ã«å½¢å¼ã‚’æ•´ãˆã‚‹
        hw = int(adapted_features_256d.shape[1] ** 0.5)
        srcs = [adapted_features_256d.permute(0, 2, 1).reshape(adapted_features_256d.shape[0], 256, hw, hw)]
        masks = [torch.zeros(s.shape[0], s.shape[2], s.shape[3], device=self.device, dtype=torch.bool) for s in srcs]
        pos_embeds = [self.pos_embed_generator(NestedTensor(s, m)).to(s.dtype) for s, m in zip(srcs, masks)]

        # 4. ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        tokenizer = self.grounding_dino.tokenizer
        tokenized = tokenizer(text_prompts, padding='longest', return_tensors="pt").to(self.device)
        bert_output = self.grounding_dino.bert(**tokenized)
        text_features_768d = bert_output['last_hidden_state']
        projected_text_features = self.grounding_dino.feat_map(text_features_768d)
        
        text_dict = {
            "encoded_text": projected_text_features,
            "text_token_mask": tokenized["attention_mask"].bool(),
        }

        # 5. GroundingDINOã®Transformerã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’èåˆ
        #    (ã“ã®éƒ¨åˆ†ãŒã€å­¦ç¿’ã—ãŸã„æ ¸å¿ƒéƒ¨åˆ†ã§ã™)
        hs, _, _, _, _ = self.grounding_dino.transformer(srcs, masks, None, pos_embeds, None, text_dict=text_dict)
        
        # 6. æå¤±è¨ˆç®—ã®ãŸã‚ã«ã€äºˆæ¸¬çµæœã‚’æ•´å½¢ã—ã¦å‡ºåŠ›
        output_class = self.grounding_dino.class_embed(hs)
        output_coord = self.grounding_dino.bbox_embed(hs).sigmoid()
        
        # æå¤±é–¢æ•°ãŒæœŸå¾…ã™ã‚‹è¾æ›¸å½¢å¼ã§è¿”ã™
        return {'pred_logits': output_class[-1], 'pred_boxes': output_coord[-1]}

# --- ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def load_components(cfg, device='cuda'):
    """
    äº‹å‰å­¦ç¿’æ¸ˆã¿ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    """
    print("ğŸ”„ Loading components...")
    
    # ImageBind
    image_encoder, _ = imagebind_huge(pretrained=True)
    image_encoder.eval().to(device)
    print("   âœ… ImageBind model loaded.")
    
    # LLM
    llm = AutoModelForCausalLM.from_pretrained(cfg.model.llm.pretrained_path, torch_dtype=torch.float16).eval().to(device)
    tokenizer = AutoTokenizer.from_pretrained(cfg.model.llm.pretrained_path, use_fast=False)
    print("   âœ… LLM and Tokenizer loaded.")
    
    # GroundingDINO
    gd_model = load_model(cfg.model.gdino.config_path, cfg.model.gdino.weights_path).eval().to(device)
    print("   âœ… GroundingDINO model loaded.")
    
    return image_encoder, llm, tokenizer, gd_model